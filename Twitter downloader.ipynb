{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Script was modified for our use from the original source: https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Tweet-Lookup/get_tweets_with_bearer_token.py\n",
    "\n",
    "In order for this script to work, one should replace <your_bearer_token> in auth function with their own private Twitter API bearer token."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "bearer_token = open(os.path.join('./TwitterBearerToken.txt'), encoding=\"utf-8\").read()\n",
    "\n",
    "DATASETS_FOLDER = \"/Users/slavkoz/OneDrive - Univerza v Ljubljani/Datasets/Offensive language datasets/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_lists_of_ids(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_url(post_ids):\n",
    "    tweet_fields = \"tweet.fields=text\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    ids = f\"ids={','.join(post_ids)}\"\n",
    "    # You can adjust ids to include a single Tweets.\n",
    "    # Or you can add to up to 100 comma-separated IDs\n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    #print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "def get_tweets(post_ids):\n",
    "    url = create_url(post_ids)\n",
    "    json_response = connect_to_endpoint(url)\n",
    "    #print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "    \n",
    "    tweets = []\n",
    "    if \"data\" in json_response:\n",
    "        for tweet in json_response[\"data\"]:\n",
    "            tweets.append([tweet[\"id\"], re.sub(r'\\s+',' ', tweet[\"text\"])])\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "def saveDF(results, filename):\n",
    "    df = pd.DataFrame(results, columns =['id', 'text'])\n",
    "    df.to_csv(filename, index = False)\n",
    "\n",
    "def retrieve_tweets(tweet_ids, filename):\n",
    "    results = []\n",
    "    for tweet_lst in get_lists_of_ids(tweet_ids, 100):\n",
    "        tweets = get_tweets(tweet_lst)\n",
    "        results.extend(tweets)\n",
    "        time.sleep(3)\n",
    "        print(f\"Retrieved {len(results)} tweets, input {len(tweet_lst)}, output {len(tweets)}\")\n",
    "    saveDF(results, filename)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 18\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '18/18_hatespeechtwitter.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "tweet_ids = list(map(str, df[\"tweet_id\"].tolist()))\n",
    "\n",
    "retrieve_tweets(tweet_ids, '18_retrieved_tweets.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 20\n",
    "benevolents = open(os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/benevolent_sexist.tsv'), 'r').readlines()\n",
    "benevolents = list(map(lambda x: re.sub(r'\\n$','', x), benevolents))\n",
    "retrieve_tweets(benevolents, '20_retrieved_benevolent_tweets.csv')\n",
    "\n",
    "hostiles = open(os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/hostile_sexist.tsv'), 'r').readlines()\n",
    "hostiles = list(map(lambda x: re.sub(r'\\n$','', x), hostiles))\n",
    "retrieve_tweets(hostiles, '20_retrieved_hostile_tweets.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 29\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '29/29_NAACL_SRW_2016.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "tweet_ids = list(map(str, df[\"tweet_id\"].tolist()))\n",
    "\n",
    "retrieve_tweets(tweet_ids, '29_retrieved_tweets.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 30\n",
    "data = open(os.path.join(DATASETS_FOLDER, '30/30_NLP_CSS_2016.csv'), 'r').readlines()\n",
    "data = list(map(lambda x: re.sub(r'\\n$','', x), data))\n",
    "data = list(map(lambda x: x.split('\\t'), data))\n",
    "data = list(map(lambda x: [x[0], x[1]], data))[1:] # tweet_id, expert - columns\n",
    "\n",
    "tweet_ids = list(map(lambda x: x[0], data))\n",
    "retrieve_tweets(tweet_ids, '30_retrieved_tweets.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:offensive-language-organization]",
   "language": "python",
   "name": "conda-env-offensive-language-organization-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}