{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LteYCMwRP9YV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import codecs\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertModel, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from pytorch_pretrained_bert import WEIGHTS_NAME, CONFIG_NAME\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHQtiROabM3y",
    "outputId": "833638b1-8fb0-4037-a04a-236ab9dd2132"
   },
   "outputs": [],
   "source": [
    "input_model = \"models/hate_bert\"\n",
    "output_model = \"./models/bert_test1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDhviBe8IT3H"
   },
   "source": [
    "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "02jeEXgTksiX",
    "outputId": "497dc34a-1e99-4fa1-b16a-59255d4fd734"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7G6K8p3ORej"
   },
   "outputs": [],
   "source": [
    "rn.seed(501)\n",
    "np.random.seed(501)\n",
    "torch.manual_seed(501)\n",
    "torch.cuda.manual_seed(501)\n",
    "\n",
    "# max lengrh of a sentence, fed into the network\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMv8Io5jIkXb"
   },
   "source": [
    "**Load Dataset**\n",
    "\n",
    "We'll use The semeval dataset for single sentence classification. It's a set of sentences labeled as offensive correct or not. The data is as follows:\n",
    "\n",
    "Column 1: the id of the sentence\n",
    "\n",
    "Column 2: the tweet text\t\n",
    "\n",
    "Column 3: Label : Sub-task A - Offensive language identification\n",
    "\n",
    "Column 4: Label : Sub-task B - Automatic categorization of offense types\n",
    "\n",
    "Column 5: Label : Sub-task C - Offense target identification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vQHdj1GTgeY"
   },
   "outputs": [],
   "source": [
    "###Load training data: offenseval 2019\n",
    "\n",
    "import pandas as pd\n",
    "DATASET_PATH = \"/home/slavkoz/Datasets/Offensive language datasets/15_OLID/\"\n",
    "df_offenseval = pd.read_csv(DATASET_PATH + 'olid-training-v1.0.tsv', delimiter ='\\t')\n",
    "X_offenseval = df_offenseval['tweet'].values\n",
    "y_offenseval = df_offenseval['subtask_a'].values\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVwEWSeAPMtS"
   },
   "outputs": [],
   "source": [
    "### load test data: offenseval 2019 (for testing)\n",
    "\n",
    "df_offenseval_test = pd.read_csv(DATASET_PATH + \"OLID-testset-levela.tsv\", delimiter = '\\t')\n",
    "X_offenseval_test = df_offenseval_test['tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQuG96Uuc0H_"
   },
   "outputs": [],
   "source": [
    "ddft_test_label = pd.read_csv(DATASET_PATH + 'OLID-labels-levela.csv', delimiter =',', header =None)\n",
    "y_offenseval_test = ddft_test_label[ddft_test_label.columns[1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZLL-vKqQpXR"
   },
   "outputs": [],
   "source": [
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_offenseval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9uyKtJRclMY"
   },
   "outputs": [],
   "source": [
    "sentences_test = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_offenseval_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej5gx6yqK0dy"
   },
   "source": [
    "**Inputs**\n",
    "\n",
    "Next, import the BERT tokenizer, used to convert our text into tokens that correspond to BERT's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgldpeeyRYPB",
    "outputId": "07ca20ef-8ca1-4156-af57-1ae0d68698a1"
   },
   "outputs": [],
   "source": [
    "### Tokenizer from HateBERT is used here  ### input_model: directory where tokenizer is present\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(input_model, do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os5H5SrndBUg",
    "outputId": "167f670b-f5c6-489f-acff-b3ef8da755cb"
   },
   "outputs": [],
   "source": [
    "tokenized_texts_test = [tokenizer.tokenize(sent) for sent in sentences_test]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62q8rXPELBgu"
   },
   "source": [
    "BERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
    "\n",
    "*   input ids: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n",
    "\n",
    "*   segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
    "\n",
    "*   attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
    "\n",
    "*   labels: a single value of 1 or 0. In our task 1 means \"offensive\" and 0 means \"not offensive\"\n",
    "\n",
    "Although we can have variable length input sentences, BERT does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length.\n",
    "\n",
    "To \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n",
    "\n",
    "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
    "\n",
    "We pad and truncate our sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) pad_sequences is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvN3EXZDT5-n"
   },
   "outputs": [],
   "source": [
    "# Use the tokenizer to convert the tokens to their index numbers in the vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgvBvC2GdK0K"
   },
   "outputs": [],
   "source": [
    "input_ids_test = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwMK-ok1T6BW"
   },
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnZdzMTtdOw5"
   },
   "outputs": [],
   "source": [
    "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1SQ_jSlT6Gb"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGGkIcwFdS_e"
   },
   "outputs": [],
   "source": [
    "attention_masks_test = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_test.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfKCWUGqVus6"
   },
   "outputs": [],
   "source": [
    "def encode_label_bin(y, predicted_label):\n",
    "    choose = lambda l : 1 if l == predicted_label else 0\n",
    "    return [choose(l) for l in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2F7tlLMVxrb"
   },
   "outputs": [],
   "source": [
    "# Indexing Labels\n",
    "y = encode_label_bin(y_offenseval, 'OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFcqPGXQdYkZ"
   },
   "outputs": [],
   "source": [
    "y_test = encode_label_bin(y_offenseval_test, 'OFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWGSTo6FT6Jw"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD801bHEUmwX"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubkp4yh3dfx5"
   },
   "outputs": [],
   "source": [
    "test_inputs = torch.LongTensor(input_ids_test)\n",
    "test_labels = torch.LongTensor(y_test)\n",
    "test_masks = torch.LongTensor(attention_masks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5QvMxPVUmzL"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADBD_ZmTdw9z"
   },
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwdK-_MxaD3b"
   },
   "source": [
    "**Train Model**\n",
    "\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "**Structure of Fine-Tuning Model** \n",
    "\n",
    "we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"offensive/non-offensive\" that are then fed into cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Srksg7_enu0r",
    "outputId": "5d489b3f-036d-4157-8c6f-663aa6650726"
   },
   "outputs": [],
   "source": [
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3CHSTcqEnCuV",
    "outputId": "89e144a8-f604-4fb7-8fcb-cad30aafa619"
   },
   "outputs": [],
   "source": [
    "#Memory release\n",
    "y, x, pooled = None, None, None\n",
    "torch.cuda.empty_cache()\n",
    "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sygPpIRcnaOY",
    "outputId": "84d93721-f8dd-4d8c-b65b-242eec2372bd"
   },
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()\n",
    " str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0y-mgWwUUm1b",
    "outputId": "949aab70-cc22-4282-8911-f770262d440b"
   },
   "outputs": [],
   "source": [
    "### The HateBERT model is loaded here  ### input_model: directory where tokenizer is present\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(input_model,num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf5zrX_kdGZA"
   },
   "source": [
    "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
    "\n",
    "For the purposes of fine-tuning, the authors recommend the following hyperparameter ranges:\n",
    "\n",
    "*   Batch size: 16, 32\n",
    "*   Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "*   Number of epochs: 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2p8FihpbUm-Y"
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgzaCOc2Um7G",
    "outputId": "6884ea8c-3930-4e9a-d3f7-198ac2f8379f"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1iyaUZXdc_v"
   },
   "source": [
    "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. At each pass we need to:\n",
    "\n",
    "**Training loop:**\n",
    "\n",
    "Tell the model to compute gradients by setting the model in train mode\n",
    "Unpack our data inputs and labels\n",
    "Load data onto the GPU for acceleration\n",
    "Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
    "Forward pass (feed input data through the network)\n",
    "Backward pass (backpropagation)\n",
    "Tell the network to update parameters with optimizer.step()\n",
    "Track variables for monitoring progress\n",
    "\n",
    "**Evalution loop:**\n",
    "\n",
    "Tell the model not to compute gradients by setting th emodel in evaluation mode\n",
    "Unpack our data inputs and labels\n",
    "Load data onto the GPU for acceleration\n",
    "Forward pass (feed input data through the network)\n",
    "Compute loss on our validation data and track variables for monitoring progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IC8-enTpipD2"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_gTf_cdivcd",
    "outputId": "509694e6-5855-4070-9032-83fee16fb74c"
   },
   "outputs": [],
   "source": [
    "# Time-consuming code here\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "Y=[]\n",
    "Z=[]\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    Y=Y+list(pred_flat)\n",
    "    Z=Z+list(labels_flat)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7J9pUQqqi6qI",
    "outputId": "5ca9fd2f-9604-45d1-a0ac-cfdb0ef89e14"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 micro: %.2f%%\" % (f1_score(Y, Z, average='micro')*100))\n",
    "print(\"F1 macro: %.2f%%\" % (f1_score(Y, Z, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Bh-fhJyTRI7",
    "outputId": "e432a397-7ebb-4d39-a4ed-d3e3c43f0b33"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(Y, Z)*100))\n",
    "print(\"F1: %.2f%%\" % (f1_score(Y, Z, average='macro')*100))\n",
    "print(\"Precission: %.2f%%\" % (precision_score(Y, Z)*100))\n",
    "print(\"Recall: %.2f%%\" % (recall_score(Y, Z)*100))\n",
    "print(classification_report(Y, Z))\n",
    "print(confusion_matrix(Y, Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YZAUwJDcMvn"
   },
   "outputs": [],
   "source": [
    "### Testing the model on offenseval test set\n",
    "model.eval()\n",
    "hatebert_predicted = []\n",
    "all_logits = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(test_dataloader):\n",
    "\n",
    "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        logits = model(token_ids, masks)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss = loss_func(logits, labels)\n",
    "        numpy_logits = logits.cpu().detach().numpy()\n",
    "        #print (numpy_logits)\n",
    "        #print (np.argmax(numpy_logits, 1))\n",
    "        hatebert_predicted += list(np.argmax(numpy_logits, 1))\n",
    "        all_logits += list(numpy_logits[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ea76_K4ee-k",
    "outputId": "483f8ca5-7730-4c5d-efe1-d222e51c60da"
   },
   "outputs": [],
   "source": [
    "print(\"F1 macro: %.2f%%\" % (f1_score(hatebert_predicted, y_test, average='macro')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p5PeztoR6hh0",
    "outputId": "48a658b9-3291-4b37-8600-69bf9e6b10c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(hatebert_predicted, y_test)*100))\n",
    "print(\"F1: %.2f%%\" % (f1_score(hatebert_predicted, y_test, average='macro')*100))\n",
    "print(\"Precission: %.2f%%\" % (precision_score(hatebert_predicted, y_test)*100))\n",
    "print(\"Recall: %.2f%%\" % (recall_score(hatebert_predicted, y_test)*100))\n",
    "print(classification_report(hatebert_predicted, y_test))\n",
    "print(confusion_matrix(hatebert_predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
    "\n",
    "# create folder if not there\n",
    "Path(output_model).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If we have a distributed model, save only the encapsulated model\n",
    "# (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_model, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_model, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Re-load the saved model and vocabulary\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(output_model, do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(output_model,num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "original_HateBERT_finetuned.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:offensive-language-organization]",
   "language": "python",
   "name": "conda-env-offensive-language-organization-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
