{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "import pandas as pd\n",
                "import re\n",
                "import json\n",
                "\n",
                "import emoji"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "source": [
                "DATASETS_FOLDER = \"/Users/slavkoz/OneDrive - Univerza v Ljubljani/Datasets/Offensive language datasets/\"\n",
                "\n",
                "TEXT_ONLY_DF = pd.DataFrame(columns=[\"text\"])\n",
                "\n",
                "def concat_text_df(df):\n",
                "    print(f\"\\t{len(df)} lines\")   \n",
                "    #print(\"\\n\".join(df[\"text\"].tail(5).tolist())) \n",
                "    return pd.concat([TEXT_ONLY_DF, df[[\"text\"]]], ignore_index=True, axis=0)\n",
                "\n",
                "def remove_html_tags(text):\n",
                "    \"\"\"Remove html tags from a string\"\"\"\n",
                "    import re\n",
                "    clean = re.compile('<.*?>')\n",
                "    return re.sub(clean, '', text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# DATASET 01\n",
                "print(\"Dataset 01 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '01_jigsaw-toxic-comment-classification-challenge/train.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'comment_text': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '01_jigsaw-toxic-comment-classification-challenge/test.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'comment_text': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 01 loading ...\n",
                        "\t159571 lines\n",
                        "\t153164 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "# DATASET 02\n",
                "print(\"Dataset 02 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '02_davidsons_dataset.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 02 loading ...\n",
                        "\t24783 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "source": [
                "# DATASET 03\n",
                "print(\"Dataset 03 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '03_HASOC_2019_english_dataset/english_dataset.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 03 loading ...\n",
                        "\t5852 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "# DATASET 04\n",
                "print(\"Dataset 04 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '04_Waseems_dataset_detect_hate_speech_data.csv')\n",
                "df = pd.read_csv(dataset_path, sep='|')\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 04 loading ...\n",
                        "\t20894 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "source": [
                "# DATASET 06\n",
                "print(\"Dataset 06 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '06_Reynolds formspring/formspring_data.csv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'post': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 06 loading ...\n",
                        "\t12773 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "source": [
                "# DATASET 08\n",
                "\n",
                "#TODO: files are password protected\n",
                "#print(\"Dataset 08 loading ...\")\n",
                "#dataset_path = os.path.join(DATASETS_FOLDER, '08_AMI_IBEREVAL2018')\n",
                "#df = pd.read_csv(dataset_path, sep='\\t')\n",
                "#df = df.rename(columns={'post': 'text'})\n",
                "#TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "#dfTrain = pd.read_csv(\"../data/iberEval/en_AMI_TrainingSet.csv\", sep=\";\")\n",
                "#    mysog_tweets = dfTrain[dfTrain[\"misogyny_category\"] != \"0\"]\n",
                "#    return list(mysog_tweets[\"tweet\"]), list(mysog_tweets[\"misogyny_category\"])"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "# DATASET 9\n",
                "print(\"Dataset 9 loading ...\")\n",
                "\n",
                "pd.DataFrame(columns=[\"text\"])\n",
                "allPosts = list()\n",
                "with open(os.path.join(DATASETS_FOLDER, '09_MMHS150K/MMHS150K_GT.json'), 'rb') as file:\n",
                "    df = json.load(file)\n",
                "#mapping = ['none', 'racist', 'sexist', 'homophobic', 'religious', 'other']\n",
                "for val in df.values():\n",
                "    #labels = np.unique(val['labels']) \n",
                "    allPosts.extend([val['tweet_text']])\n",
                "    #allTypes.extend([mapping[label] for label in labels])\n",
                "\n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 9 loading ...\n",
                        "\t149823 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "source": [
                "# DATASET 10\n",
                "print(\"Dataset 10 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '10_jig-quian gab.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "\n",
                "allPosts = list()\n",
                "for text, idx in zip(df['text'], df['hate_speech_idx']):\n",
                "    posts = re.split('[0-9]+. \\t+', text)\n",
                "    posts[0] = posts[0][3:]\n",
                "    idx_list = re.split(r'\\[|\\]', str(idx))\n",
                "    types = ['hateful' if str(i) in idx_list else 'none'\n",
                "                for i in range(1, len(posts) + 1)]\n",
                "    allPosts.extend(posts)\n",
                "    #allTypes.extend(types)\n",
                "\n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 10 loading ...\n",
                        "\t33776 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "# DATASET 11\n",
                "print(\"Dataset 11 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '11_jig-quian reddit.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "\n",
                "allPosts = list()\n",
                "for text, idx in zip(df['text'], df['hate_speech_idx']):\n",
                "    posts = re.split('[0-9]+. \\t+', text)\n",
                "    posts[0] = posts[0][3:]\n",
                "    idx_list = re.split(r'\\[|\\]', str(idx))\n",
                "    types = ['hateful' if str(i) in idx_list else 'none'\n",
                "                for i in range(1, len(posts) + 1)]\n",
                "    allPosts.extend(posts)\n",
                "    #allTypes.extend(types)\n",
                "\n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 11 loading ...\n",
                        "\t22309 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "# DATASET 12\n",
                "print(\"Dataset 12 loading ...\")\n",
                "\n",
                "allPosts = list()\n",
                "allTypes = list()\n",
                "\n",
                "annotations = pd.read_csv(os.path.join(DATASETS_FOLDER, '12_white supremacist forum/annotations_metadata.csv'))\n",
                "for index, row in annotations.iterrows():\n",
                "    if row['label'] in ['hate', 'noHate']:\n",
                "        text = open(os.path.join(DATASETS_FOLDER, '12_white supremacist forum/all_files/' + str(row['file_id']) + '.txt'), 'r', encoding='utf-8').read()\n",
                "        label = row['label']\n",
                "        allPosts.append(text)\n",
                "        allTypes.append(label)\n",
                "\n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 12 loading ...\n",
                        "\t10703 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "source": [
                "# DATASET 13\n",
                "print(\"Dataset 13 loading ...\")\n",
                "\n",
                "allPosts = list()\n",
                "allTypes = list()\n",
                "with open(os.path.join(DATASETS_FOLDER, '13_CONAN.json'), encoding=\"utf-8\") as json_file:\n",
                "    data = json.load(json_file)\n",
                "for record in data['conan']:\n",
                "    allPosts.append(record['hateSpeech'])\n",
                "    allTypes.append(record['hsType'])\n",
                "    \n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 13 loading ...\n",
                        "\t14988 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "# DATASET 14\n",
                "print(\"Dataset 14 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '14_ousidhoum - en_dataset_with_stop_words.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 14 loading ...\n",
                        "\t5647 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "# DATASET 15\n",
                "print(\"Dataset 15 loading ...\")\n",
                "\n",
                "#dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/OLID-labels-levela.csv')\n",
                "#df = pd.read_csv(dataset_path, names=['id', 'label'])\n",
                "#set(df['label'].tolist())\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/OLID-testset-levela.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/olid-training-v1.0.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "\n",
                "#"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 15 loading ...\n",
                        "\t860 lines\n",
                        "\t13240 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "# DATASET 16\n",
                "print(\"Dataset 16 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '16_fox-news-all-comments.txt')\n",
                "df = pd.read_csv(dataset_path, names=['text'], sep=':')\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 16 loading ...\n",
                        "\t1528 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "source": [
                "# DATASET 17\n",
                "print(\"Dataset 17 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '17_trac2/eng/trac2_eng_train.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'Text': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '17_trac2/eng/trac2_eng_dev.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "df = df.rename(columns={'Text': 'text'})"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 17 loading ...\n",
                        "\t4263 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "# DATASET 19\n",
                "print(\"Dataset 19 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '19_Online Harassment Dataset/onlineHarassmentDataset.tdf')\n",
                "df = pd.read_csv(dataset_path, sep='\\t', encoding=\"ISO-8859-1\")\n",
                "df = df.rename(columns={'Tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 19 loading ...\n",
                        "\t20360 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "source": [
                "# DATASET 21\n",
                "print(\"Dataset 21 loading ...\")\n",
                "\n",
                "allPosts = list()\n",
                "allTypes = list()\n",
                "with open(os.path.join(DATASETS_FOLDER, '21_HateXplain-dataset.json'), encoding=\"utf-8\") as json_file:\n",
                "    data = json.load(json_file)\n",
                "\n",
                "for item in data:    \n",
                "    allPosts.append(\" \".join(data[item]['post_tokens']))\n",
                "#    allTypes.append(TODO)\n",
                "    \n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 21 loading ...\n",
                        "\t20148 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "source": [
                "# DATASET 25\n",
                "print(\"Dataset 25 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '25_2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv')\n",
                "df = pd.read_csv(dataset_path)\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 25 loading ...\n",
                        "\t40623 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "source": [
                "# DATASET 26\n",
                "\n",
                "# proposals for classification\n",
                "#\"macro-norm-violations-n10-t0-misogynistic-slurs.csv\" - Comments that use misogynistic slurs --> term 'slur'  \n",
                "#\"macro-norm-violations-n15-t2-hatespeech-racist-homophobic.csv\" - Comments containing hate speech that is racist or homophobic --> term 'homophobic'  \n",
                "#\"macro-norm-violations-n15-t3-abusing-and-criticisizing-mods.csv\" - Comments abusing and criticisizng moderators. --> term 'abuse'\n",
                "\n",
                "print(\"Dataset 26 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n10-t0-misogynistic-slurs.csv')\n",
                "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n15-t2-hatespeech-racist-homophobic.csv')\n",
                "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n15-t3-abusing-and-criticisizing-mods.csv')\n",
                "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 26 loading ...\n",
                        "\t5059 lines\n",
                        "\t5059 lines\n",
                        "\t5059 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "# DATASET 27\n",
                "\n",
                "print(\"Dataset 27 loading ...\")\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_train.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'Tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_test.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'Tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)\n",
                "\n",
                "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_val.tsv')\n",
                "df = pd.read_csv(dataset_path, sep='\\t')\n",
                "df = df.rename(columns={'Tweet': 'text'})\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 27 loading ...\n",
                        "\t5218 lines\n",
                        "\t1000 lines\n",
                        "\t500 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "# DATASET 28\n",
                "print(\"Dataset 28 loading ...\")\n",
                "\n",
                "allPosts = list()\n",
                "with open(os.path.join(DATASETS_FOLDER, '28_LoL_dataset/lol_anonymized_ann.txt'), 'r', encoding=\"utf-8\") as file:\n",
                "    ann = file.read()[1:-1]\n",
                "    ann = ann.split('),(')\n",
                "    ann = [(entry.split(',')[0], entry.split(',')[1]) for entry in ann]\n",
                "with open(os.path.join(DATASETS_FOLDER, '28_LoL_dataset/lol_anonymized_posts.txt'), 'r', encoding=\"utf-8\") as file:\n",
                "    posts = ','.join(file.readlines())[1:-1]\n",
                "    posts = posts.split('),(')\n",
                "for post in posts:\n",
                "    split = post.split(',')\n",
                "    topic_id = split[0]\n",
                "    post_number = split[1]\n",
                "    html_message = ''.join(split[3:-1])\n",
                "    allPosts.extend([remove_html_tags(html_message)])\n",
                "    #allTypes.extend(['cyberbullying' if (topic_id, post_number) in ann else 'none'])\n",
                "\n",
                "df = pd.DataFrame(allPosts, columns =['text'])\n",
                "TEXT_ONLY_DF = concat_text_df(df)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Dataset 28 loading ...\n",
                        "\t17342 lines\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "print(f\"FINAL DATASET:\\n\\t{len(TEXT_ONLY_DF)} lines\")\n",
                "\n",
                "print(\"Doing basic preprocessing ...\")\n",
                "\n",
                "def basic_preprocessing(row):\n",
                "    text = row[\"text\"]\n",
                "    text = text.replace('\"', '')\n",
                "    text = re.sub(r'^\\'','', text)\n",
                "    text = re.sub(r'\\'$','', text)\n",
                "    text = re.sub(r'^\\s+','', text)\n",
                "    text = re.sub(r'\\s+$','', text)\n",
                "    text = re.sub(r'\\s+',' ', text)\n",
                "    \n",
                "    text = emoji.demojize(text)\n",
                "    text = text.replace('::', ' ')\n",
                "    row[\"text\"] = text  \n",
                "    return row\n",
                "\n",
                "TEXT_ONLY_DF = TEXT_ONLY_DF.apply(lambda x : basic_preprocessing(x), axis=1)\n",
                "print(\"... basic preprocessing done.\")\n",
                "\n",
                "print(\"Saving full datasets ...\")\n",
                "TEXT_ONLY_DF.to_csv('full_textOnly_dataset.csv', index = False)\n",
                "print(\"... datasets saved.\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "FINAL DATASET:\n",
                        "\t754542 lines\n",
                        "Doing basic preprocessing ...\n",
                        "... basic preprocessing done.\n",
                        "Saving full datasets ...\n",
                        "... datasets saved.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "print(\"Doing cleaning ...\")\n",
                "CLEANED_DF = pd.DataFrame(TEXT_ONLY_DF[\"text\"],columns=[\"text\"])\n",
                "\n",
                "def cleaning(row):\n",
                "    text = row[\"text\"]\n",
                "    text = text.replace('RT', '')\n",
                "    text = text.replace('…', '')\n",
                "    text = text.replace('Q:', '')\n",
                "    text = text.replace('A:', '')\n",
                "    text = text.replace('“', '')\n",
                "    text = text.replace('”', '')\n",
                "    text = text.replace('``', '')\n",
                "    text = text.replace('\\'\\'', '')\n",
                "    text = text.replace('\\\\n', ' ')\n",
                "    text = text.replace('\\\\\\'', '\\'')\n",
                "    text = re.sub(r'@[A-Za-z0-9\\-_]+:?', '', text)\n",
                "    text = re.sub(r'https?://\\S+', '', text)    \n",
                "    text = re.sub(r'<.*?>', '', text) # remove html tags        \n",
                "    text = re.sub(r'&.*?;', '', text) # remove &sometihngttags        \n",
                "    text = re.sub(r'&lt;|&gt;|&amp;', '', text)   #remove tags\n",
                "    text = re.sub(r'\\s+',' ', text)    \n",
                "    row[\"text\"] = text\n",
                "    return row\n",
                "\n",
                "CLEANED_DF = CLEANED_DF.apply(lambda x : cleaning(x), axis=1)\n",
                "print(\"... cleaning done.\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Doing cleaning ...\n",
                        "... cleaning done.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "print(\"Saving cleaned dataset ...\")\n",
                "CLEANED_DF.to_csv('full_textOnly_cleaned_dataset.csv', index = False, header=False)\n",
                "print(\"... dataset saved.\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Saving cleaned dataset ...\n",
                        "... dataset saved.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}