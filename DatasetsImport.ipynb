{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import emoji\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "DATASETS_FOLDER = \"/Users/slavkoz/OneDrive - Univerza v Ljubljani/Datasets/Offensive language datasets/\"\n",
    "#DATASETS_FOLDER = \"/home/slavkoz/Datasets/Offensive language datasets/\"\n",
    "\n",
    "TEXT_ONLY_DF = pd.DataFrame(columns=[\"text\"])\n",
    "\n",
    "def concat_text_df(df):\n",
    "    print(f\"\\t{len(df)} lines\")   \n",
    "    #print(\"\\n\".join(df[\"text\"].tail(5).tolist())) \n",
    "    return pd.concat([TEXT_ONLY_DF, df[[\"text\"]]], ignore_index=True, axis=0)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def create_tweet_dict(df):\n",
    "    tweet_id_to_text = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        tweet_id_to_text[row[\"id\"]] = row[\"text\"]\n",
    "    return tweet_id_to_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# DATASET 01\n",
    "print(\"Dataset 01 loading ...\")\n",
    "full_dataset = []\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '01_jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'comment_text': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"toxic\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"toxic\"])\n",
    "    if row[\"severe_toxic\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"severe_toxic\"])\n",
    "    if row[\"obscene\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"obscene\"])\n",
    "    if row[\"threat\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"threat\"])\n",
    "    if row[\"insult\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"insult\"])\n",
    "    if row[\"identity_hate\"] == 1:\n",
    "        full_dataset.append([1, row[\"text\"], \"identity_hate\"])\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '01_jigsaw-toxic-comment-classification-challenge/test.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'comment_text': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "# Only some examples from test set used for evaluation\n",
    "mapper = {\"toxic\": set(), \"severe_toxic\": set(), \"obscene\": set(), \"threat\": set(), \"insult\": set(), \"identity_hate\": set()}\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '01_jigsaw-toxic-comment-classification-challenge/test_labels.csv')\n",
    "df_test_labels = pd.read_csv(dataset_path)\n",
    "for index, row in df_test_labels.iterrows():\n",
    "    if row[\"toxic\"] == 1:\n",
    "        mapper[\"toxic\"].add(row[\"id\"])\n",
    "    if row[\"severe_toxic\"] == 1:\n",
    "        mapper[\"severe_toxic\"].add(row[\"id\"])\n",
    "    if row[\"obscene\"] == 1:\n",
    "        mapper[\"obscene\"].add(row[\"id\"])\n",
    "    if row[\"threat\"] == 1:\n",
    "        mapper[\"threat\"].add(row[\"id\"])\n",
    "    if row[\"insult\"] == 1:\n",
    "        mapper[\"insult\"].add(row[\"id\"])\n",
    "    if row[\"identity_hate\"] == 1:\n",
    "        mapper[\"identity_hate\"].add(row[\"id\"])\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if row[\"id\"] in mapper[\"toxic\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"toxic\"])\n",
    "    if row[\"id\"] in mapper[\"severe_toxic\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"severe_toxic\"])\n",
    "    if row[\"id\"] in mapper[\"obscene\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"obscene\"])\n",
    "    if row[\"id\"] in mapper[\"threat\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"threat\"])\n",
    "    if row[\"id\"] in mapper[\"insult\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"insult\"])\n",
    "    if row[\"id\"] in mapper[\"identity_hate\"]:\n",
    "        full_dataset.append([1, row[\"text\"], \"identity_hate\"])\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df1[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 01 loading ...\n",
      "\t159571 lines\n",
      "\t153164 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "toxic            21384\n",
       "obscene          12140\n",
       "insult           11304\n",
       "identity_hate     2117\n",
       "severe_toxic      1962\n",
       "threat             689\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# DATASET 02\n",
    "print(\"Dataset 02 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '02_davidsons_dataset.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "mapper = {0: \"hate_speech\", 1: \"offensive_language\", 2: \"neither\"}\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    full_dataset.append([2, row[\"text\"], mapper[row[\"class\"]]])\n",
    "\n",
    "df2 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df2[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 02 loading ...\n",
      "\t24783 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "offensive_language    19190\n",
       "neither                4163\n",
       "hate_speech            1430\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# DATASET 03\n",
    "print(\"Dataset 03 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '03_HASOC_2019_english_dataset/english_dataset.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "mapper = {\"NONE\": \"none\", \"HATE\": \"hate\", \"OFFN\": \"offensive\", \"PRFN\": \"profane\"}\n",
    "for index, row in df.iterrows():\n",
    "    full_dataset.append([3, row[\"text\"], mapper[row[\"task_2\"]]])\n",
    "\n",
    "df3 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df3[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 03 loading ...\n",
      "\t5852 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none         3591\n",
       "hate         1143\n",
       "profane       667\n",
       "offensive     451\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# DATASET 04\n",
    "print(\"Dataset 04 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '04_Waseems_dataset_detect_hate_speech_data.csv')\n",
    "df = pd.read_csv(dataset_path, sep='|')\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    full_dataset.append([4, row[\"text\"], row[\"label\"]])\n",
    "\n",
    "df4 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df4[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 04 loading ...\n",
      "\t20894 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none          16844\n",
       "sexism         3963\n",
       "homophobia       87\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# DATASET 06\n",
    "print(\"Dataset 06 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '06_Reynolds formspring/formspring_data.csv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'post': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    labels = [row[\"ans1\"], row[\"ans2\"], row[\"ans3\"]]\n",
    "    if labels.count(\"Yes\") > 1:\n",
    "        full_dataset.append([6, row[\"text\"], \"cyberbullying\"])\n",
    "    else:\n",
    "        full_dataset.append([6, row[\"text\"], \"none\"])\n",
    "\n",
    "df6 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df6[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 06 loading ...\n",
      "\t12773 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none             11997\n",
       "cyberbullying      776\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# DATASET 07\n",
    "print(\"Dataset 07 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '07_founta/hatespeech_text_label_vote.csv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t', header=None)\n",
    "df = df.rename(columns={0: 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    full_dataset.append([7, row[\"text\"], row[1]])\n",
    "\n",
    "df7 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df7[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 07 loading ...\n",
      "\t99996 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "normal     53851\n",
       "abusive    27150\n",
       "spam       14030\n",
       "hateful     4965\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# DATASET 08\n",
    "\n",
    "#TODO: files are password protected\n",
    "#print(\"Dataset 08 loading ...\")\n",
    "#dataset_path = os.path.join(DATASETS_FOLDER, '08_AMI_IBEREVAL2018')\n",
    "#df = pd.read_csv(dataset_path, sep='\\t')\n",
    "#df = df.rename(columns={'post': 'text'})\n",
    "#TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "#dfTrain = pd.read_csv(\"../data/iberEval/en_AMI_TrainingSet.csv\", sep=\";\")\n",
    "#    mysog_tweets = dfTrain[dfTrain[\"misogyny_category\"] != \"0\"]\n",
    "#    return list(mysog_tweets[\"tweet\"]), list(mysog_tweets[\"misogyny_category\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# DATASET 9\n",
    "print(\"Dataset 9 loading ...\")\n",
    "\n",
    "pd.DataFrame(columns=[\"text\"])\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "with open(os.path.join(DATASETS_FOLDER, '09_MMHS150K/MMHS150K_GT.json'), 'rb') as file:\n",
    "    df = json.load(file)\n",
    "mapping = ['none', 'racist', 'sexist', 'homophobic', 'religious', 'other']\n",
    "for val in df.values():\n",
    "    labels = np.unique(val['labels']) \n",
    "    allPosts.extend([val['tweet_text']])\n",
    "    allTypes.extend([mapping[label] for label in labels])\n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([9, post, typ])\n",
    "\n",
    "df9 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df9[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 9 loading ...\n",
      "\t149823 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none          84573\n",
       "racist        30290\n",
       "other         14942\n",
       "sexist        11322\n",
       "homophobic     7397\n",
       "religious      1299\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# DATASET 10\n",
    "print(\"Dataset 10 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '10_jig-quian gab.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "for text, idx in zip(df['text'], df['hate_speech_idx']):\n",
    "    posts = re.split('[0-9]+. \\t+', text)\n",
    "    posts[0] = posts[0][3:]\n",
    "    idx_list = re.split(r'\\[|\\]', str(idx))\n",
    "    types = ['hateful' if str(i) in idx_list else 'none'\n",
    "                for i in range(1, len(posts) + 1)]\n",
    "    allPosts.extend(posts)\n",
    "    allTypes.extend(types)\n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([10, post, typ])\n",
    "\n",
    "df10 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df10[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 10 loading ...\n",
      "\t33776 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none       24840\n",
       "hateful     8936\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# DATASET 11\n",
    "print(\"Dataset 11 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '11_jig-quian reddit.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "for text, idx in zip(df['text'], df['hate_speech_idx']):\n",
    "    posts = re.split('[0-9]+. \\t+', text)\n",
    "    posts[0] = posts[0][3:]\n",
    "    idx_list = re.split(r'\\[|\\]', str(idx))\n",
    "    types = ['hateful' if str(i) in idx_list else 'none'\n",
    "                for i in range(1, len(posts) + 1)]\n",
    "    allPosts.extend(posts)\n",
    "    allTypes.extend(types)\n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([11, post, typ])\n",
    "\n",
    "df11 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df11[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 11 loading ...\n",
      "\t22309 lines\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "none       19330\n",
       "hateful     2979\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# DATASET 12\n",
    "print(\"Dataset 12 loading ...\")\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "\n",
    "annotations = pd.read_csv(os.path.join(DATASETS_FOLDER, '12_white supremacist forum/annotations_metadata.csv'))\n",
    "for index, row in annotations.iterrows():\n",
    "    if row['label'] in ['hate', 'noHate']:\n",
    "        text = open(os.path.join(DATASETS_FOLDER, '12_white supremacist forum/all_files/' + str(row['file_id']) + '.txt'), 'r', encoding='utf-8').read()\n",
    "        label = row['label']\n",
    "        allPosts.append(text)\n",
    "        allTypes.append(label)\n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([12, post, typ])\n",
    "\n",
    "df12 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df12[\"label\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 12 loading ...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/slavkoz/Datasets/Offensive language datasets/12_white supremacist forum/all_files/12834493_1.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-579019c3dbdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noHate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASETS_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'12_white supremacist forum/all_files/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mallPosts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/slavkoz/Datasets/Offensive language datasets/12_white supremacist forum/all_files/12834493_1.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 13\n",
    "print(\"Dataset 13 loading ...\")\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "with open(os.path.join(DATASETS_FOLDER, '13_CONAN.json'), encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "for record in data['conan']:\n",
    "    allPosts.append(record['hateSpeech'])\n",
    "    allTypes.append(record['hsType'])\n",
    "    \n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([13, post, typ])\n",
    "\n",
    "df13 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df13[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 14\n",
    "print(\"Dataset 14 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '14_ousidhoum - en_dataset_with_stop_words.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([14, row[\"text\"], row[\"sentiment\"]])    \n",
    "\n",
    "df14 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df14[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 15\n",
    "print(\"Dataset 15 loading ...\")\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/OLID-labels-levela.csv')\n",
    "df = pd.read_csv(dataset_path, names=['id', 'label'])\n",
    "id_to_label = dict()\n",
    "for index, row in df.iterrows(): \n",
    "    id_to_label[row[\"id\"]] = row[\"label\"]\n",
    "\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/OLID-testset-levela.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "mapper = {\"NOT\": \"non-offensive\", \"OFF\": \"offensive\"}\n",
    "for index, row in df.iterrows():  \n",
    "    full_dataset.append([15, row[\"text\"], mapper[id_to_label[row[\"id\"]]]]) \n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '15_OLID/olid-training-v1.0.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([15, row[\"text\"], mapper[row[\"subtask_a\"]]])  \n",
    "\n",
    "df15 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df15[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 16\n",
    "print(\"Dataset 16 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '16_fox-news-all-comments.txt')\n",
    "df = pd.read_csv(dataset_path, names=['label', 'text'], sep=':')\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():    \n",
    "    if row[\"label\"] == 1:\n",
    "        full_dataset.append([16, row[\"text\"], \"hateful\"])  \n",
    "    else:\n",
    "        full_dataset.append([16, row[\"text\"], \"non-hateful\"])  \n",
    "\n",
    "df16 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df16[\"label\"].value_counts()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 17\n",
    "print(\"Dataset 17 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '17_trac2/eng/trac2_eng_train.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'Text': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "mapper = {\"NAG\": \"non-aggressive\", \"OAG\": \"overtly-aggressive\", \"CAG\": \"covertly-aggressive\"}\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([17, row[\"text\"], mapper[row[\"Sub-task A\"]]])  \n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '17_trac2/eng/trac2_eng_dev.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "df = df.rename(columns={'Text': 'text'})\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([17, row[\"text\"], mapper[row[\"Sub-task A\"]]])  \n",
    "\n",
    "df17 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df17[\"label\"].value_counts()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 18\n",
    "print(\"Dataset 18 loading ...\")\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '18/18_retrieved_tweets.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "tweet_id_to_text = create_tweet_dict(df)\n",
    "tweet_id_to_text\n",
    "\n",
    "\n",
    "full_dataset = []\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '18/18_hatespeechtwitter.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "for index, row in df.iterrows():\n",
    "    tweet_id = row['tweet_id']\n",
    "    if tweet_id in tweet_id_to_text:\n",
    "        full_dataset.append([18, tweet_id_to_text[tweet_id], row['maj_label']])\n",
    "\n",
    "df18 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df18[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 19\n",
    "print(\"Dataset 19 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '19_Online Harassment Dataset/onlineHarassmentDataset.tdf')\n",
    "df = pd.read_csv(dataset_path, sep='\\t', encoding=\"ISO-8859-1\")\n",
    "df = df.rename(columns={'Tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "mapper = {\"N\": \"non-harrasment\", \"H\": \"harrasment\"}\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([19, row[\"text\"], mapper[row['Code']]])\n",
    "\n",
    "df19 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df19[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 20\n",
    "print(\"Dataset 20 loading ...\")\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/20_retrieved_benevolent_tweets.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "tweet_id_to_text_b = create_tweet_dict(df)\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/20_retrieved_hostile_tweets.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "tweet_id_to_text_h = create_tweet_dict(df)\n",
    "\n",
    "full_dataset = []\n",
    "benevolents = open(os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/benevolent_sexist.tsv'), 'r').readlines()\n",
    "benevolents = list(map(lambda x: re.sub(r'\\n$','', x), benevolents))\n",
    "for benevolent in benevolents:\n",
    "    if int(benevolent) in tweet_id_to_text_b:\n",
    "        full_dataset.append([20, tweet_id_to_text_b[int(benevolent)], 'benevolent_sexist'])\n",
    "\n",
    "hostiles = open(os.path.join(DATASETS_FOLDER, '20_NLP_CSS_2017-master/hostile_sexist.tsv'), 'r').readlines()\n",
    "hostiles = list(map(lambda x: re.sub(r'\\n$','', x), hostiles))\n",
    "for hostile in hostiles:\n",
    "    if int(hostile) in tweet_id_to_text_h:\n",
    "        full_dataset.append([20, tweet_id_to_text_h[int(hostile)], 'hostile_sexist'])\n",
    "\n",
    "\n",
    "\n",
    "df20 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df20[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 21\n",
    "print(\"Dataset 21 loading ...\")\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "with open(os.path.join(DATASETS_FOLDER, '21_HateXplain-dataset.json'), encoding=\"utf-8\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "for item in data:    \n",
    "    allPosts.append(\" \".join(data[item]['post_tokens']))\n",
    "    labels = list(map(lambda x: x[\"label\"], data[item]['annotators']))\n",
    "    if labels.count('hatespeech') >= len(labels)/2:\n",
    "        allTypes.append('hatespeech')\n",
    "    elif labels.count('offensive') >= len(labels)/2:\n",
    "        allTypes.append('offensive')\n",
    "    else: \n",
    "        allTypes.append('normal')\n",
    "\n",
    "  \n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([21, post, typ])\n",
    "\n",
    "df21 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df21[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 25\n",
    "print(\"Dataset 25 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '25_2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([25, row[\"text\"], row['label']])\n",
    "\n",
    "df25 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df25[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 26\n",
    "\n",
    "print(\"Dataset 26 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n10-t0-misogynistic-slurs.csv')\n",
    "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([26, row[\"text\"], \"misogyny-slur\"])\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n15-t2-hatespeech-racist-homophobic.csv')\n",
    "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([26, row[\"text\"], \"racist-homophobic\"])\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '26_Reddit_norm_violations/macro-norm-violations-n15-t3-abusing-and-criticisizing-mods.csv')\n",
    "df = pd.read_csv(dataset_path, names=[\"text\"])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    full_dataset.append([26, row[\"text\"], \"abuse\"])\n",
    "\n",
    "df26 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df26[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 27\n",
    "\n",
    "print(\"Dataset 27 loading ...\")\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_train.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'Tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for index, row in df.iterrows():    \n",
    "    if row[\"Majority\"] >= 3:\n",
    "        full_dataset.append([27, row[\"text\"], \"non-vulgar\"])\n",
    "    else:\n",
    "        full_dataset.append([27, row[\"text\"], \"vulgar\"])\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_test.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'Tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    if row[\"Majority\"] >= 3:\n",
    "        full_dataset.append([27, row[\"text\"], \"non-vulgar\"])\n",
    "    else:\n",
    "        full_dataset.append([27, row[\"text\"], \"vulgar\"])\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '27_VulgarTwitter/cleaned_data_val.tsv')\n",
    "df = pd.read_csv(dataset_path, sep='\\t')\n",
    "df = df.rename(columns={'Tweet': 'text'})\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    if row[\"Majority\"] >= 3:\n",
    "        full_dataset.append([27, row[\"text\"], \"non-vulgar\"])\n",
    "    else:\n",
    "        full_dataset.append([27, row[\"text\"], \"vulgar\"])\n",
    "\n",
    "df27 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df27[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 28\n",
    "print(\"Dataset 28 loading ...\")\n",
    "\n",
    "allPosts = list()\n",
    "allTypes = list()\n",
    "with open(os.path.join(DATASETS_FOLDER, '28_LoL_dataset/lol_anonymized_ann.txt'), 'r', encoding=\"utf-8\") as file:\n",
    "    ann = file.read()[1:-1]\n",
    "    ann = ann.split('),(')\n",
    "    ann = [(entry.split(',')[0], entry.split(',')[1]) for entry in ann]\n",
    "with open(os.path.join(DATASETS_FOLDER, '28_LoL_dataset/lol_anonymized_posts.txt'), 'r', encoding=\"utf-8\") as file:\n",
    "    posts = ','.join(file.readlines())[1:-1]\n",
    "    posts = posts.split('),(')\n",
    "for post in posts:\n",
    "    split = post.split(',')\n",
    "    topic_id = split[0]\n",
    "    post_number = split[1]\n",
    "    html_message = ''.join(split[3:-1])\n",
    "    allPosts.extend([remove_html_tags(html_message)])\n",
    "    allTypes.extend(['cyberbullying' if (topic_id, post_number) in ann else 'none'])\n",
    "\n",
    "df = pd.DataFrame(allPosts, columns =['text'])\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "full_dataset = []\n",
    "for post, typ in zip(allPosts, allTypes):\n",
    "    full_dataset.append([28, post, typ])\n",
    "\n",
    "df28 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df28[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 29\n",
    "print(\"Dataset 29 loading ...\")\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '29/29_retrieved_tweets.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "tweet_id_to_text = create_tweet_dict(df)\n",
    "tweet_id_to_text\n",
    "\n",
    "\n",
    "full_dataset = []\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '29/29_NAACL_SRW_2016.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "for index, row in df.iterrows():\n",
    "    tweet_id = row['tweet_id']\n",
    "    if tweet_id in tweet_id_to_text:\n",
    "        full_dataset.append([29, tweet_id_to_text[tweet_id], row['class']])\n",
    "\n",
    "df29 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df29[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATASET 30\n",
    "print(\"Dataset 30 loading ...\")\n",
    "\n",
    "dataset_path = os.path.join(DATASETS_FOLDER, '30/30_retrieved_tweets.csv')\n",
    "df = pd.read_csv(dataset_path)\n",
    "TEXT_ONLY_DF = concat_text_df(df)\n",
    "\n",
    "tweet_id_to_text = create_tweet_dict(df)\n",
    "tweet_id_to_text\n",
    "\n",
    "\n",
    "full_dataset = []\n",
    "data = open(os.path.join(DATASETS_FOLDER, '30/30_NLP_CSS_2016.csv'), 'r').readlines()\n",
    "data = list(map(lambda x: re.sub(r'\\n$','', x), data))\n",
    "data = list(map(lambda x: x.split('\\t'), data))\n",
    "data = list(map(lambda x: [x[0], x[1]], data))[1:] # tweet_id, expert - columns\n",
    "df = pd.DataFrame(data, columns=[\"tweet_id\", \"class\"])\n",
    "for index, row in df.iterrows():\n",
    "    tweet_id = row['tweet_id']\n",
    "    if int(tweet_id) in tweet_id_to_text:\n",
    "        full_dataset.append([30, tweet_id_to_text[int(tweet_id)], row['class']])\n",
    "\n",
    "df30 = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "df30[\"label\"].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"FINAL DATASET:\\n\\t{len(TEXT_ONLY_DF)} lines\")\n",
    "\n",
    "print(\"Doing basic preprocessing ...\")\n",
    "\n",
    "def basic_preprocessing(row):\n",
    "    text = row[\"text\"]\n",
    "    text = text.replace('\"', '')\n",
    "    text = re.sub(r'^\\'','', text)\n",
    "    text = re.sub(r'\\'$','', text)\n",
    "    text = re.sub(r'^\\s+','', text)\n",
    "    text = re.sub(r'\\s+$','', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace('::', ' ')\n",
    "    row[\"text\"] = text  \n",
    "    return row\n",
    "\n",
    "TEXT_ONLY_DF = TEXT_ONLY_DF.apply(lambda x : basic_preprocessing(x), axis=1)\n",
    "print(\"... basic preprocessing done.\")\n",
    "\n",
    "print(\"Saving full datasets ...\")\n",
    "TEXT_ONLY_DF.to_csv('full_textOnly_dataset.csv', index = False)\n",
    "print(\"... datasets saved.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Doing cleaning ...\")\n",
    "CLEANED_DF = pd.DataFrame(TEXT_ONLY_DF[\"text\"],columns=[\"text\"])\n",
    "\n",
    "def cleaning(row):\n",
    "    text = row[\"text\"]\n",
    "    text = text.replace('RT', '')\n",
    "    text = text.replace('…', '')\n",
    "    text = text.replace('Q:', '')\n",
    "    text = text.replace('A:', '')\n",
    "    text = text.replace('“', '')\n",
    "    text = text.replace('”', '')\n",
    "    text = text.replace('``', '')\n",
    "    text = text.replace('\\'\\'', '')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    text = text.replace('\\\\\\'', '\\'')\n",
    "    text = re.sub(r'@[A-Za-z0-9\\-_]+:?', '', text)\n",
    "    text = re.sub(r'https?://\\S+', '', text)    \n",
    "    text = re.sub(r'<.*?>', '', text) # remove html tags        \n",
    "    text = re.sub(r'&.*?;', '', text) # remove &sometihngttags        \n",
    "    text = re.sub(r'&lt;|&gt;|&amp;', '', text)   #remove tags\n",
    "    text = re.sub(r'\\s+',' ', text)    \n",
    "    row[\"text\"] = text\n",
    "    return row\n",
    "\n",
    "CLEANED_DF = CLEANED_DF.apply(lambda x : cleaning(x), axis=1)\n",
    "print(\"... cleaning done.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Saving cleaned dataset ...\")\n",
    "CLEANED_DF.to_csv('full_textOnly_cleaned_dataset.csv', index = False, header=False)\n",
    "print(\"... dataset saved.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f\"FULL CLASSIFICATION DATASET:\")\n",
    "\n",
    "print(\"Combining dataset ...\")\n",
    "FULL_CLASS_DF = pd.DataFrame(full_dataset, columns=[\"corpus_id\", \"text\", \"label\"])\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df1], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df2], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df3], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df4], ignore_index=True, axis=0)\n",
    "#FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df5], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df6], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df7], ignore_index=True, axis=0)\n",
    "#FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df8], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df9], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df10], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df11], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df12], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df13], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df14], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df15], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df16], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df17], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df18], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df19], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df20], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df21], ignore_index=True, axis=0)\n",
    "#FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df22], ignore_index=True, axis=0)\n",
    "#FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df23], ignore_index=True, axis=0)\n",
    "#FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df24], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df25], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df26], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df27], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df28], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df29], ignore_index=True, axis=0)\n",
    "FULL_CLASS_DF = pd.concat([FULL_CLASS_DF, df30], ignore_index=True, axis=0)\n",
    "\n",
    "print(f\"\\nCounts by corpus id: \\n{FULL_CLASS_DF['corpus_id'].value_counts()}\")\n",
    "print(f\"\\nCounts by label type: \\n{FULL_CLASS_DF['label'].value_counts()}\\n\")\n",
    "print(\"... combining dataset done.\")\n",
    "\n",
    "print(\"Doing basic preprocessing ...\")\n",
    "FULL_CLASS_DF = FULL_CLASS_DF.apply(lambda x : basic_preprocessing(x), axis=1)\n",
    "print(\"... basic preprocessing done.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Saving full classification dataset ...\")\n",
    "FULL_CLASS_DF.to_csv('full_classification_dataset.csv', index = False)\n",
    "print(\"... dataset saved.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fa48241555dd6574f80adc4b257cccd8c253aedd249236aba361b71bc201e96"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('offensive-language-organization': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}